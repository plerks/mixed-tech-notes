预先准备好服务器，最好是在一个内网内的至少3台服务器，也可以节点都是公网ip，但是集群节点间的通信走公网可能会受限于运营商网络带宽。

部署的时候需要关闭防火墙和swap，至少防火墙不关部署确实会出问题。

我试验的环境是centos，首先添加k8s的yum仓库，增加/etc/yum.repos.d/kubenetes.repo
```
[kubernetes]
name=kubernetes
baseurl=https://mirrors.tuna.tsinghua.edu.cn/kubernetes/yum/repos/kubernetes-el7-$basearch
enabled=1
gpgcheck=0 #使用镜像仓，这里方便起见直接关了gpgcheck
repo_gpgcheck=0
```
安装kubeadm,kubelet,kubectl:
```
yum install -y kubelet-1.21.3 kubeadm-1.21.3 kubectl-1.21.3
systemctl enable kubelet && systemctl start kubelet
```
用的不是最新版本，用最新版本kubeadm init的时候出现:
```
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
```
的问题，而且k8s较新版本用的containerd作为[容器运行时](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/)，安装也需要改动。

[安装docker](https://www.runoob.com/docker/ubuntu-docker-install.html):`curl -sSL https://get.daocloud.io/docker | sh && systemctl enable docker && systemctl start docker`。

拉取镜像(参考<https://blog.csdn.net/narcissus2_/article/details/119423389>)，新建k8sImagePull.sh:
```
#!/bin/bash
images=(
 kube-apiserver:v1.21.3
 kube-controller-manager:v1.21.3
 kube-scheduler:v1.21.3
 kube-proxy:v1.21.3
 pause:3.2
 etcd:3.4.13-0
)
for imageName in ${images[@]} ; do
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName}
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName} k8s.gcr.io/${imageName}
  docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/${imageName}
done
docker pull coredns/coredns:1.8.0
docker tag coredns/coredns:1.8.0 registry.aliyuncs.com/google_containers/coredns:v1.8.0
docker rmi coredns/coredns:1.8.0
```
运行以拉取k8s镜像(通过镜像仓拉取并在本地重新tag，非被墙环境下直接拉取就行)。

以上在每个节点上都需要做。

在选定为master的节点上：
```
kubeadm init --image-repository=registry.aliyuncs.com/google_containers  --pod-network-cidr=10.244.0.0/16	 --service-cidr=10.96.0.0/12
```

配置master上的kubectl:
```
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

join的命令可以用`kubeadm token create --print-join-command`获取，然后在node上进行join。

以上完成之后`kubectl get node`可以看到节点，但是是NotReady状态，需要安装网络插件:
```
kubectl apply -f "https://docs.projectcalico.org/manifests/calico.yaml"
```

安装[nginx ingress-controller](https://kubernetes.github.io/ingress-nginx/deploy/):
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.4.0/deploy/static/provider/cloud/deploy.yaml
```
(网络不通的话先在本机下载这个yaml，然后复制到服务器上。此外apply时需要提前用别的方式拉容器(比如自己下载后推到自己dockerhub，不过要把nginx ingress controller的yaml里容器tag的sha256去掉，因为自己重新tag后推到自己dockerhub会变)，然后在node上把tag改回来，否则`kubectl get pod -n ingress-nginx`会发现镜像拉取不到)

nginx ingress controller会在ingress-nginx命名空间下安装一个LoadBalancer类型的service，非云厂商环境，需要用NodePort入站:

首先修改[NodePort的范围](https://kuboard.cn/install/install-node-port-range.html)为0-32767。

然后：`kubectl edit svc ingress-nginx-controller -n ingress-nginx`,把spec里的NodePort改成80和443,type改成NodePort。(其实可以不改类型为NodePort，因为LoadBalancer既是NodePort，也是ClusterIP，不改的话`kubectl get svc ingress-nginx-controller -o wide -n ingress-nginx`会发现LoadBalancer的EXTERNAL-IP会一直pending)

用这3个文件测试访问：

ingress-nginx.yaml:
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  namespace: ingress-nginx-test
spec:
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: front-cluster-ip
                port:
                  number: 80

```

front-cluster-ip.yaml:
```
apiVersion: v1
kind: Service
metadata:
  name: front-cluster-ip
  namespace: ingress-nginx-test
spec:
  type: ClusterIP
  selector:
    component: front
  ports:
    - port: 80
      targetPort: 8000

```
front-deployment.yaml:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: front-deployment
  namespace: ingress-nginx-test
spec:
  replicas: 1
  selector:
    matchLabels:
      component: front
  template:
    metadata:
      labels:
        component: front
    spec:
      containers:
        - name: front
          image: registry.cn-beijing.aliyuncs.com/zuozewei/sample-webapp:1.0
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "250m"
              memory: "512Mi"
          ports:
            - containerPort: 8000
```
上面的测试容器来自：<https://cloud.tencent.com/developer/article/1766165>

### 手动设置一个外部的负载均衡
可以用nginx设置一个外部的负载均衡，具体来说，用一台集群外部的带公网ip的服务器，将请求负载均衡到集群内部的各个Node上(与云厂商环境下的LoadBalancer一致)。
配置大致方式是：
```
http {
    upstream webhost {
        server 192.168.1.200:80;
        server 192.168.1.201:80;
        server 192.168.1.202:80;
    }
    server {
        listen 80 default_server;
        location / {
            proxy_pass http://webhost;
        }
    }
    其它内容
}
```

### 搭建网络卷
// TODO 还未实践搭k8s的存储

### 卸载集群
先kubectl delete各个Node，然后每个节点上`kubeadm reset`后，按照提示：
```
The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
```
参考[这个](https://www.jianshu.com/p/a93b38ca520f)。需要：
```
rm -rf /etc/cni/net.d/

如果是iptables模式，需要
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

如果是ipvs模式,需要
ipvsadm --clear

rm -rf $HOME/.kube
```